{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['catastici']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(0, 3183, 'liberal', 'campi', 'casa e bottega da barbier', 70, 'campo vicino alla chiesa')]\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../data/catastici.db\")\n",
    "\n",
    "# test DB\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM catastici LIMIT 1;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_query(sql_query):\n",
    "    \"\"\"clean the output\"\"\"\n",
    "    # change to list\n",
    "    sql_query_list = ast.literal_eval(sql_query)\n",
    "    \n",
    "    # split on ;\n",
    "    sql_query_list = [query.split(';')[0].strip() + ';' for query in sql_query_list]\n",
    "    # sql_query_list = [query.split(';')[0].split('[/SQL]')[0].strip() + ';' for query in sql_query_list]\n",
    "    \n",
    "    # replace ' with ''\n",
    "    sql_query_list = [re.sub(r\"([a-z])'([a-z])\",r\"\\1''\\2\", query) for query in sql_query_list]\n",
    "\n",
    "    return sql_query_list\n",
    "\n",
    "def check_sql_executability(query, db):\n",
    "    try:\n",
    "        return db.run(query)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "     \n",
    "def find_most_common_answer(answers):\n",
    "    answers = [\"ERROR\" if \"error\" in answer else answer for answer in answers]\n",
    "    most_common_answer, most_common_count = Counter(answers).most_common(1)[0]\n",
    "    if most_common_answer == \"ERROR\":\n",
    "        if most_common_count == 4:\n",
    "            return most_common_answer\n",
    "        else:\n",
    "            return Counter(answers).most_common(2)[1][0]\n",
    "\n",
    "    return most_common_answer\n",
    "\n",
    "def clean_answer(answer, to_replace = ['[', ']', '(', ',)', \"'\", ')']):\n",
    "    pattern = '|'.join(map(re.escape, to_replace))\n",
    "    cleaned_answer = re.sub(pattern, '', answer)\n",
    "    return [ans.strip() for ans in cleaned_answer.split(',')]\n",
    "\n",
    "def ngram_overlap(true_answer, generated_answer):\n",
    "    \"\"\"Calculates 1-gram overlap\"\"\"\n",
    "    # Convert lists to sets to remove duplicates\n",
    "    true_set = set(true_answer)\n",
    "    generated_set = set(generated_answer)\n",
    "    \n",
    "    # Calculate the intersection of the sets\n",
    "    overlap_count = len(true_set.intersection(generated_set))\n",
    "    \n",
    "    # Calculate the percentage of overlap\n",
    "    overlap_percentage = overlap_count / len(true_set) if len(true_set) > 0 else 0.0\n",
    "    \n",
    "    return round(overlap_percentage, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "EM       193\n",
       "ERROR      7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "query_res = pd.read_csv('./test_data_generated_0.csv')\n",
    "\n",
    "# clean output\n",
    "for idx, row in query_res.iterrows():\n",
    "    query_list_clean = clean_query(row['generated_query'])\n",
    "    final_out = None\n",
    "    answers = []\n",
    "    for out in query_list_clean:\n",
    "        answers.append(check_sql_executability(out, db))\n",
    "    answer = find_most_common_answer(answers)\n",
    "    \n",
    "    if answer != \"ERROR\":\n",
    "        final_out = query_list_clean[answers.index(answer)]\n",
    "    if final_out == None:\n",
    "        final_out = query_list_clean[0]\n",
    "        answer = check_sql_executability(final_out, db)\n",
    "    query_res.loc[idx,'generated_answer'] = answer\n",
    "    query_res.loc[idx,'generated_query'] = final_out\n",
    "    \n",
    "query_res.loc[(query_res['generated_answer'].str.contains(\"error\")), 'output'] = 'ERROR'\n",
    "query_res.loc[(query_res['generated_answer']==query_res['true_answer']), 'output'] = 'EM'\n",
    "query_res['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "True     169\n",
       "False    131\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wrong = query_res[query_res.output.isna()]\n",
    "query_wrong.true_answer.fillna(\"\",inplace=True)\n",
    "\n",
    "query_wrong['generated_answer_clean'] = query_wrong['generated_answer'].apply(clean_answer)\n",
    "query_wrong['true_answer_clean'] = query_wrong['true_answer'].apply(clean_answer)\n",
    "\n",
    "n_gram = []\n",
    "for _, row in query_wrong.iterrows():\n",
    "    n_gram.append(ngram_overlap(row['true_answer_clean'], row['generated_answer_clean']))\n",
    "query_wrong['n_gram_overlap'] = n_gram\n",
    "query_wrong['output'] = query_wrong['n_gram_overlap']>0.33\n",
    "\n",
    "query_wrong['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "EM    282\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "query_res = pd.read_csv('./test_data_generated_3.csv')\n",
    "\n",
    "# clean output\n",
    "for idx, row in query_res.iterrows():\n",
    "    query_list_clean = clean_query(row['generated_query'])\n",
    "    final_out = None\n",
    "    answers = []\n",
    "    for out in query_list_clean:\n",
    "        answers.append(check_sql_executability(out, db))\n",
    "    answer = find_most_common_answer(answers)\n",
    "    \n",
    "    if answer != \"ERROR\":\n",
    "        final_out = query_list_clean[answers.index(answer)]\n",
    "    if final_out == None:\n",
    "        final_out = query_list_clean[0]\n",
    "        answer = check_sql_executability(final_out, db)\n",
    "    query_res.loc[idx,'generated_answer'] = answer\n",
    "    query_res.loc[idx,'generated_query'] = final_out\n",
    "    \n",
    "query_res.loc[(query_res['generated_answer'].str.contains(\"error\")), 'output'] = 'ERROR'\n",
    "query_res.loc[(query_res['generated_answer']==query_res['true_answer']), 'output'] = 'EM'\n",
    "query_res['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "False    115\n",
       "True     103\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wrong = query_res[query_res.output.isna()]\n",
    "query_wrong.true_answer.fillna(\"\",inplace=True)\n",
    "\n",
    "query_wrong['generated_answer_clean'] = query_wrong['generated_answer'].apply(clean_answer)\n",
    "query_wrong['true_answer_clean'] = query_wrong['true_answer'].apply(clean_answer)\n",
    "\n",
    "n_gram = []\n",
    "for _, row in query_wrong.iterrows():\n",
    "    n_gram.append(ngram_overlap(row['true_answer_clean'], row['generated_answer_clean']))\n",
    "query_wrong['n_gram_overlap'] = n_gram\n",
    "query_wrong['output'] = query_wrong['n_gram_overlap']>0.33\n",
    "\n",
    "query_wrong['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output_3\n",
       "EM       282\n",
       "False    115\n",
       "True     103\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['question_id','level_len','level_nest','question','true_query','true_answer','matched_contents','generated_query','generated_answer','output']\n",
    "query_3 = pd.concat([query_res[query_res['output'].notna()], query_wrong], axis=0)[columns].rename({\n",
    "        'generated_query':'generated_query_3', \n",
    "        'generated_answer':'generated_answer_3', \n",
    "        'output':'output_3'},axis=1\n",
    "    )\n",
    "query_3['output_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "EM    287\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "query_res = pd.read_csv('./test_data_generated_5.csv')\n",
    "\n",
    "# clean output\n",
    "for idx, row in query_res.iterrows():\n",
    "    query_list_clean = clean_query(row['generated_query'])\n",
    "    final_out = None\n",
    "    answers = []\n",
    "    for out in query_list_clean:\n",
    "        answers.append(check_sql_executability(out, db))\n",
    "    answer = find_most_common_answer(answers)\n",
    "    \n",
    "    if answer != \"ERROR\":\n",
    "        final_out = query_list_clean[answers.index(answer)]\n",
    "    if final_out == None:\n",
    "        final_out = query_list_clean[0]\n",
    "        answer = check_sql_executability(final_out, db)\n",
    "    query_res.loc[idx,'generated_answer'] = answer\n",
    "    query_res.loc[idx,'generated_query'] = final_out\n",
    "    \n",
    "query_res.loc[(query_res['generated_answer'].str.contains(\"error\")), 'output'] = 'ERROR'\n",
    "query_res.loc[(query_res['generated_answer']==query_res['true_answer']), 'output'] = 'EM'\n",
    "query_res['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "True     110\n",
       "False    103\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wrong = query_res[query_res.output.isna()]\n",
    "query_wrong.true_answer.fillna(\"\",inplace=True)\n",
    "\n",
    "query_wrong['generated_answer_clean'] = query_wrong['generated_answer'].apply(clean_answer)\n",
    "query_wrong['true_answer_clean'] = query_wrong['true_answer'].apply(clean_answer)\n",
    "\n",
    "n_gram = []\n",
    "for _, row in query_wrong.iterrows():\n",
    "    n_gram.append(ngram_overlap(row['true_answer_clean'], row['generated_answer_clean']))\n",
    "query_wrong['n_gram_overlap'] = n_gram\n",
    "query_wrong['output'] = query_wrong['n_gram_overlap']>0.33\n",
    "\n",
    "query_wrong['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output_5\n",
       "EM       287\n",
       "True     110\n",
       "False    103\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['question_id','level_len','level_nest','question','true_query','true_answer','matched_contents','generated_query','generated_answer','output']\n",
    "query_5 = pd.concat([query_res[query_res['output'].notna()], query_wrong], axis=0)[columns].rename({\n",
    "        'generated_query':'generated_query_5', \n",
    "        'generated_answer':'generated_answer_5', \n",
    "        'output':'output_5'},axis=1\n",
    "    )\n",
    "query_5['output_5'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "EM    292\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "query_res = pd.read_csv('./test_data_generated_7.csv')\n",
    "\n",
    "# clean output\n",
    "for idx, row in query_res.iterrows():\n",
    "    query_list_clean = clean_query(row['generated_query'])\n",
    "    final_out = None\n",
    "    answers = []\n",
    "    for out in query_list_clean:\n",
    "        answers.append(check_sql_executability(out, db))\n",
    "    answer = find_most_common_answer(answers)\n",
    "    \n",
    "    if answer != \"ERROR\":\n",
    "        final_out = query_list_clean[answers.index(answer)]\n",
    "    if final_out == None:\n",
    "        final_out = query_list_clean[0]\n",
    "        answer = check_sql_executability(final_out, db)\n",
    "    query_res.loc[idx,'generated_answer'] = answer\n",
    "    query_res.loc[idx,'generated_query'] = final_out\n",
    "    \n",
    "query_res.loc[(query_res['generated_answer'].str.contains(\"error\")), 'output'] = 'ERROR'\n",
    "query_res.loc[(query_res['generated_answer']==query_res['true_answer']), 'output'] = 'EM'\n",
    "query_res['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "True     105\n",
       "False    103\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wrong = query_res[query_res.output.isna()]\n",
    "query_wrong.true_answer.fillna(\"\",inplace=True)\n",
    "\n",
    "query_wrong['generated_answer_clean'] = query_wrong['generated_answer'].apply(clean_answer)\n",
    "query_wrong['true_answer_clean'] = query_wrong['true_answer'].apply(clean_answer)\n",
    "\n",
    "n_gram = []\n",
    "for _, row in query_wrong.iterrows():\n",
    "    n_gram.append(ngram_overlap(row['true_answer_clean'], row['generated_answer_clean']))\n",
    "query_wrong['n_gram_overlap'] = n_gram\n",
    "query_wrong['output'] = query_wrong['n_gram_overlap']>0.33\n",
    "\n",
    "query_wrong['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output_7\n",
       "EM       292\n",
       "True     105\n",
       "False    103\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['question_id','level_len','level_nest','question','true_query','true_answer','matched_contents','generated_query','generated_answer','output']\n",
    "query_7 = pd.concat([query_res[query_res['output'].notna()], query_wrong], axis=0)[columns].rename({\n",
    "        'generated_query':'generated_query_7', \n",
    "        'generated_answer':'generated_answer_7', \n",
    "        'output':'output_7'},axis=1\n",
    "    )\n",
    "query_7['output_7'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-shot - 15b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "EM    306\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "query_res = pd.read_csv('./test_data_generated_5_15b.csv')\n",
    "\n",
    "# clean output\n",
    "for idx, row in query_res.iterrows():\n",
    "    query_list_clean = clean_query(row['generated_query'])\n",
    "    final_out = None\n",
    "    answers = []\n",
    "    for out in query_list_clean:\n",
    "        answers.append(check_sql_executability(out, db))\n",
    "    answer = find_most_common_answer(answers)\n",
    "    \n",
    "    if answer != \"ERROR\":\n",
    "        final_out = query_list_clean[answers.index(answer)]\n",
    "    if final_out == None:\n",
    "        final_out = query_list_clean[0]\n",
    "        answer = check_sql_executability(final_out, db)\n",
    "    query_res.loc[idx,'generated_answer'] = answer\n",
    "    query_res.loc[idx,'generated_query'] = final_out\n",
    "    \n",
    "query_res.loc[(query_res['generated_answer'].str.contains(\"error\")), 'output'] = 'ERROR'\n",
    "query_res.loc[(query_res['generated_answer']==query_res['true_answer']), 'output'] = 'EM'\n",
    "query_res['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "True     98\n",
       "False    96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wrong = query_res[query_res.output.isna()]\n",
    "query_wrong.true_answer.fillna(\"\",inplace=True)\n",
    "\n",
    "query_wrong['generated_answer_clean'] = query_wrong['generated_answer'].apply(clean_answer)\n",
    "query_wrong['true_answer_clean'] = query_wrong['true_answer'].apply(clean_answer)\n",
    "\n",
    "n_gram = []\n",
    "for _, row in query_wrong.iterrows():\n",
    "    n_gram.append(ngram_overlap(row['true_answer_clean'], row['generated_answer_clean']))\n",
    "query_wrong['n_gram_overlap'] = n_gram\n",
    "query_wrong['output'] = query_wrong['n_gram_overlap']>0.33\n",
    "\n",
    "query_wrong['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "EM       306\n",
       "True      98\n",
       "False     96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['question_id','level_len','level_nest','question','true_query','true_answer','matched_contents','generated_query','generated_answer','output']\n",
    "query_res = pd.concat([query_res[query_res['output'].notna()], query_wrong], axis=0)[columns]\n",
    "query_res['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_len\n",
      "0    0.90\n",
      "1    0.44\n",
      "Name: output_binary, dtype: float64\n",
      "level_nest\n",
      "0    0.898734\n",
      "1    0.466667\n",
      "Name: output_binary, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# difficulty\n",
    "query_res['output_binary'] = (query_res.output!=False)\n",
    "\n",
    "print(query_res.groupby('level_len')['output_binary'].mean())\n",
    "print(query_res.groupby('level_nest')['output_binary'].mean())\n",
    "query_res.drop('output_binary',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = pd.merge(query_3, query_5, on=['question_id','level_len','level_nest','question','true_query','true_answer','matched_contents'])\n",
    "query = pd.merge(query, query_7, on=['question_id','level_len','level_nest','question','true_query','true_answer','matched_contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_3\n",
      "EM       282\n",
      "False    115\n",
      "True     103\n",
      "Name: count, dtype: int64\n",
      "output_5\n",
      "EM       287\n",
      "True     110\n",
      "False    103\n",
      "Name: count, dtype: int64\n",
      "output_7\n",
      "EM       292\n",
      "True     105\n",
      "False    103\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(query_3['output_3'].value_counts())\n",
    "print(query_5['output_5'].value_counts())\n",
    "print(query_7['output_7'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_len\n",
      "0    0.8775\n",
      "1    0.4600\n",
      "Name: output_binary, dtype: float64\n",
      "level_nest\n",
      "0    0.883544\n",
      "1    0.457143\n",
      "Name: output_binary, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# difficulty\n",
    "query['output_binary'] = (query.output_5!=False)\n",
    "\n",
    "print(query.groupby('level_len')['output_binary'].mean())\n",
    "print(query.groupby('level_nest')['output_binary'].mean())\n",
    "query.drop('output_binary',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.to_csv('test_data_eval_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 - 85\n",
      "Question: What sum of rent income is produced by the elite five earners?\n",
      "True Answer: [(23954,)]\n",
      "Generated Answer: [(17038,)]\n",
      "\n",
      "True SQL:\n",
      "SELECT SUM(\"Rent_Income\") AS Total_Rent_Income\n",
      "FROM catastici\n",
      "WHERE (\"Owner_First_Name\", \"Owner_Family_Name\") IN (\n",
      "SELECT \"Owner_First_Name\", \"Owner_Family_Name\"\n",
      "FROM catastici\n",
      "ORDER BY \"Rent_Income\" DESC\n",
      "LIMIT 5\n",
      ");\n",
      "Generated SQL:\n",
      "SELECT SUM(\"Rent_Income\") AS Total_Rent_Income\n",
      "FROM catastici\n",
      "WHERE \"Owner_ID\" IN (\n",
      "\tSELECT \"Owner_ID\"\n",
      "\tFROM catastici\n",
      "\tGROUP BY \"Owner_ID\"\n",
      "\tORDER BY COUNT(\"Owner_ID\") DESC\n",
      "\tLIMIT 5\n",
      ");\n",
      "\n",
      "\n",
      "\n",
      "456 - 91\n",
      "Question: What is the number of property holders who earn over 100 ducati from rents in total?\n",
      "True Answer: [(1113,)]\n",
      "Generated Answer: [(489,)]\n",
      "\n",
      "True SQL:\n",
      "SELECT COUNT(*) AS owners_with_more_than_100_income\n",
      "FROM (\n",
      "SELECT DISTINCT \"Owner_First_Name\", \"Owner_Family_Name\"\n",
      "FROM catastici\n",
      "GROUP BY \"Owner_First_Name\", \"Owner_Family_Name\"\n",
      "HAVING SUM(\"Rent_Income\") > 100\n",
      ") AS owners_with_income_above_100;\n",
      "Generated SQL:\n",
      "SELECT COUNT(DISTINCT \"Owner_ID\") \n",
      "FROM catastici \n",
      "WHERE \"Rent_Income\" > 100;\n",
      "\n",
      "\n",
      "\n",
      "344 - 68\n",
      "Question: What percentage of the total rent income is generated by \"bottega da fabro\" type of properties?\n",
      "True Answer: [(0.03392513852764899,)]\n",
      "Generated Answer: [(0,)]\n",
      "\n",
      "True SQL:\n",
      "SELECT 100.0 * SUM(CASE WHEN \"Property_Type\" = 'bottega da fabro' THEN \"Rent_Income\" ELSE 0 END) / SUM(\"Rent_Income\") AS Fabro_Rent_Income_Percentage\n",
      "FROM catastici;\n",
      "Generated SQL:\n",
      "SELECT SUM(CASE WHEN \"Property_Type\" = 'bottega da fabro' THEN Rent_Income ELSE 0 END) / SUM(\"Rent_Income\") * 100 AS percentage\n",
      "FROM catastici;\n",
      "\n",
      "\n",
      "\n",
      "450 - 90\n",
      "Question: Do various families own properties of the same kind located at \"loco vicino la calle del paradiso\"?\n",
      "True Answer: \n",
      "Generated Answer: [(0,)]\n",
      "\n",
      "True SQL:\n",
      "SELECT \"Property_Type\", COUNT(DISTINCT \"Owner_Family_Name\") AS num_families\n",
      "FROM catastici\n",
      "WHERE \"Property_Location\" = 'loco vicino la calle del paradiso'\n",
      "GROUP BY \"Property_Type\"\n",
      "HAVING COUNT(DISTINCT \"Owner_Family_Name\") > 1\n",
      "limit 1;\n",
      "Generated SQL:\n",
      "SELECT COUNT(DISTINCT \"Owner_Family_Name\") \n",
      "FROM catastici \n",
      "WHERE \"Property_Location\" = 'loco vicino la calle del paradiso' AND \"Property_Type\" = 'casa';\n",
      "\n",
      "\n",
      "\n",
      "425 - 85\n",
      "Question: How much do the top 5 earners generate in total rental income?\n",
      "True Answer: [(23954,)]\n",
      "Generated Answer: [(548266,)]\n",
      "\n",
      "True SQL:\n",
      "SELECT SUM(\"Rent_Income\") AS Total_Rent_Income\n",
      "FROM catastici\n",
      "WHERE (\"Owner_First_Name\", \"Owner_Family_Name\") IN (\n",
      "SELECT \"Owner_First_Name\", \"Owner_Family_Name\"\n",
      "FROM catastici\n",
      "ORDER BY \"Rent_Income\" DESC\n",
      "LIMIT 5\n",
      ");\n",
      "Generated SQL:\n",
      "SELECT SUM(\"Rent_Income\") AS Total_Rent_Income\n",
      "FROM catastici\n",
      "ORDER BY Total_Rent_Income DESC\n",
      "LIMIT 5;\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in query_wrong[query_wrong.output == False].sample(5).iterrows():\n",
    "    print(f\"{idx} - {row['question_id']}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    # print(f\"N-gram: {row['n_gram_overlap']}\")\n",
    "    print(f\"True Answer: {row['true_answer']}\")\n",
    "    print(f\"Generated Answer: {row['generated_answer']}\")\n",
    "    print()\n",
    "    print('True SQL:')\n",
    "    print(row['true_query'])\n",
    "    print('Generated SQL:')\n",
    "    print(row['generated_query'])\n",
    "    print('\\n\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
