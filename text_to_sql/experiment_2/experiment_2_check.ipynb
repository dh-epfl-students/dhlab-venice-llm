{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['catastici']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[('liberal', 'campi', 'casa e bottega da barbier', 70, 'campo vicino alla chiesa')]\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../data/catastici.db\")\n",
    "\n",
    "# test DB\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM catastici LIMIT 1;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "query_res = pd.read_csv('./test_data_generated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "def clean_query(sql_query):\n",
    "    \"\"\"clean the output\"\"\"\n",
    "    # change to list\n",
    "    sql_query_list = ast.literal_eval(sql_query)\n",
    "    \n",
    "    # split on ;\n",
    "    sql_query_list = [query.split(';')[0]+';' for query in sql_query_list]\n",
    "    return sql_query_list\n",
    "\n",
    "def check_sql_executability(query, db):\n",
    "    try:\n",
    "        return db.run(query)\n",
    "    except:\n",
    "        return \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean output\n",
    "for idx, row in query_res.iterrows():\n",
    "    query_list_clean = clean_query(row['generated_query'])\n",
    "    final_out = None\n",
    "    for out in query_list_clean:\n",
    "        answer = check_sql_executability(out, db)\n",
    "        if answer != \"ERROR\":\n",
    "            final_out = out\n",
    "            break\n",
    "    if final_out == None:\n",
    "        final_out = '\\n'.join(query_list_clean)\n",
    "        answer = \"ERROR\"\n",
    "    query_res.loc[idx,'generated_answer'] = answer\n",
    "    query_res.loc[idx,'generated_query'] = final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_res[(query_res['generated_answer']!='ERROR') & (query_res['generated_answer']!=query_res['true_answer'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_res[(query_res['generated_answer']=='ERROR')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wrong -> 246<br>\n",
    "    - True -> 162\n",
    "    - Wrong -> 84 <= 12 + [123, 166, 167, 169, 175-179, 186, 209, 213, 225, 227, 256-258, 266, 276-278, 281 (uppercased), 308, 330, 338, 345-346, 348-349, 351, 370-371, 375-377, 383, 387, 388, 408, 415-419, 422-423, 441, 455-459, 461-464, 476, 479-489, 495-499]\n",
    "- Error -> 18<br>\n",
    "- Exact Match -> 230\n",
    "- True -> **392**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong Groud Truth\n",
    "17(distinct), 20, 40, 75, 95, 121, 135, 170(answer is nan), 185(answer is nan), 240, 306, 311 (do sum), 390-393 (median), 417 (this specific question), 480 (tricky median)\n",
    "\n",
    "### Ambigious questions <br>\n",
    "200 - 204, 85 - 89 (total property or single property?), 90-94, 375, 430, 433"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both correct\n",
    "- Question: What does the typical rental revenue look like for properties situated at \"al ponte di san provolo\"? -> TRUE (average), GEN (average by property type)\n",
    "- What is the range of rent incomes in \"calle de franchi\"? -> TRUE (min, max), GEN (max-min)\n",
    "\n",
    "### Wrong generation\n",
    "\n",
    "Sometimes, it puts extra filter (on Limit to avoid long answer, but it is not asked) - 3, 8, 11, 12, 13, 14, 25, 35<br>\n",
    "- It, sometimes, uses limit 1 when question is singular\n",
    "Sometimes, it confuses the feature names: i.e Rent_Income instead of Property_Type <br>\n",
    "Example questions:\n",
    "- Which properties have a rent income higher than 50 ducati? (limit 1)\n",
    "- Question: What are the different classifications of property present in \"teren alli gesouiti\"? (uses \"Property_Classification\")\n",
    "- How many properties are enumerated in the dataset? (uses \"WHERE \"Property_Type\" = 'enumerazione'\")\n",
    "- Who is the owner earning the highest rental income from a property? (does sum instead of max)\n",
    "- What is the count of unique locations where properties are situated? (does not count)\n",
    "- What does the typical rental revenue amount to for each category of the \"perina\" \"capello\" properties? (it does \"Property_Location\" = 'perina')\n",
    "- What is the total number of properties generating less than 30 ducati in rent? (does sum of rent instead of count)\n",
    "\n",
    "### Limitations - Error\n",
    "Non-existing SQL keywords, such as STDDEV: e.g. For each property location, what's the standard rental income? -> uses STDDEV instead of AVG (question is also ambigious)<br>\n",
    "Statistics, i.e. median, standard deviation, variance, ... <br>\n",
    "More than 1 arguments in Count <br>\n",
    "Non-existing feature names, when qeustion is ambigious: e.g. How many real estate properties are on lease for over \"38\"? <br>\n",
    "Example questions: \n",
    "- Could you provide the median leasing earnings across all assets in \"al ponte di san provolo\"? (uses PERCENTILE_CONT)\n",
    "- Can you tell me the total number of different property owners present in the dataset? (>2 args in Count)\n",
    "- For each property location, what's the standard rental income? (uses SDDEV)\n",
    "- How many real estate properties are on lease for over \"38\"? (uses non-existing feature)\n",
    "- Whose real estate assets are spread over the widest array of locations?\n",
    "- What is the number of households possessing assets across multiple type categories?\n",
    "- What is the average rent income variance across all locations? (wrong statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in query_res[(query_res['generated_answer']!='ERROR') & (query_res['generated_answer']!=query_res['true_answer'])].iterrows():\n",
    "    print(f\"{row['level']} - {row['question_id']} - {idx}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Answer True: {row['true_answer']}\")\n",
    "    print(f\"Answer Generated: {row['generated_answer']}\")\n",
    "    print('True SQL:')\n",
    "    print(row['true_query'])\n",
    "    print('Generated SQL:')\n",
    "    print(row['generated_query'])\n",
    "    print('\\n\\n')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
